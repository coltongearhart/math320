\documentclass{article}
\usepackage{style-notes}

\newcounter{lecnum} 	% define counter for lecture number
\renewcommand{\thepage}{\thelecnum-\arabic{page}}	% define how page number is displayed (< lecture number > - < page number >)
% define lecture header and page numbers
% NOTE: to call use \lecture{< Lecture # >, < Lecture name >, < Chapter # >, < Chapter name >, < Section #s >}
\newcommand{\lecture}[5]{

    % define headers for first page
    \thispagestyle{empty} % removes page number from page where call is made

    \setcounter{lecnum}{#1}		% set lecture counter to argument specified

    % define header box
    \begin{center}
    \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in {\textbf{MATH 320: Probability} \hfill}
       \vspace{4mm}
       \hbox to 6.28in {{\hfill \Large{Lecture #1: #2} \hfill}}
       \vspace{2mm}
       \hbox to 6.28in {\hfill Chapter #3: #4 \small{(#5)}}
      \vspace{2mm}}
    }
    \end{center}
    \vspace{4mm}
    
    % define headers for subsequent pages
    \fancyhead[LE]{\textit{#2} \hfill \thepage} 		% set left header for even pages
    \fancyhead[RO]{\hfill \thepage}		% set right header for odd pages

}

% define macros (/shortcuts)
\newcommand{\bu}[1]{\textbf{\ul{#1}}}				% shortcut bold and underline text in one command
\newcommand{\blankul}[1]{\rule[-1.5mm]{#1}{0.15mm}}	% shortcut for blank underline, where the only option needed to specify is length (# and units (cm or mm, etc.)))
\newcommand{\comp}{{\sim}}						% shortcut for tilde without extra space, using for complement
\newcommand{\ind}{\perp \!\!\! \perp}		% define independence symbol (it basically makes two orthogonal symbols very close to each other; the number of \! controls the space between each of the orthogonal symbols)


% NOTES on what didn't cover


\begin{document}

\lecture{5}{Independent Events}{1}{Probability}{1.4}

\bu{The independence of events}\bigskip

Motivation\bigskip
\begin{itemize}
    \item For certain pairs of events, the occurrence of one of them may or may not change the probability of the occurrence of the other.
    \item Example: Roll a die. $A = \{1, 2, 3, 4\}$, $B = \{1, 3, 5\}$ and $C= \{4, 5, 6\}$.
    \item[] Compute the probabilities of $P(C \mid A)$ and $P(B \mid A)$, then compare them with $P(C)$ and $P(B)$, respectively.\bigskip
    \item[] 1. $P(C \mid A) = $ \hspace{170pt} 2. $P(B \mid A) = $\vspace{70pt}
    \item How can we interpret the results above?
    \begin{enumerate}
        \item The knowledge of the occurrence of $A$ has \blankul{2cm} the probability of $C$.
        \item $P(B)$ is \blankul{7.5cm} the occurrence of $A$.
    \end{enumerate}
\end{itemize}\bigskip

Definition of independence\bigskip
\begin{itemize}
    \item Two events $A$ and $B$, are \textbf{independent} if \vspace{20pt}
    \item[] If $P(A) > 0$ and $P(B) > 0$, then $A \ind B$ $\Longleftrightarrow$ \vspace{50pt}
    \item[] Otherwise, events are said to be dependent.
    \item To check for independence, we only need to check one of the three conditions. If one is true, then all are true.\bigskip
    \item[] Example: If a fair coin is tossed twice, then $S = \{HH, HT, TH, TT\}$. Let $H1$ be the event that the first toss is a head, and $H2$ be the event that the second toss is a head. Check if $H1$ and $H2$ are independent.\vspace{60pt}
    \item Many experiments are best approached by assuming that successive trials are independent, just like successive tosses of a coin.
    \item[] There is another common problem in which independence and dependence are intuitively clear.\vspace{30pt}
    \item[] Example: Drawing cards, probabilities change if the card drawn is not replaced.
\end{itemize}\bigskip

Multiplication rule for independent events\bigskip
\begin{itemize}
    \item The general multiplication rule for any two events is \vspace{40pt}
    \item[] If $\ind$, find \blankul{1.5cm} probability by multiplying \blankul{2.5cm} probabilities.
    \item \textbf{Multiplication rule for independent events}\bigskip
    \item[] If $A$ and $B$ are independent events, $P(A \cap B) = $\bigskip
    \item This multiplication rule makes some problems very easy if independence is immediately recognized. However, it may be tricky to check for in practice. So in many problems when it is not intuitively obvious, it is simply given as an assumption.
    \item[] Example: Suppose the probability of hitting a target is 0.2 and ten shots are fired independently.
    \begin{enumerate}[(a)]
        \item What is the probability the target is hit at least once?\vspace{60pt}
        \item What is the conditional probability the target is hit twice, given that it is hit at least once?\vspace{120pt}
    \end{enumerate}
    \item Summary:
    \begin{itemize}
        \item If $A$ and $B$ are independent, can easily compute $P(A \cap B) =  P(A) \cdot P(B)$\\
        \item If $A$ and $B$ are mutually exclusive, can easily compute $P(A \cup B)$ 
    \end{itemize}
\end{itemize}\bigskip

Theorems
\bigskip
\begin{itemize}
    \item If $A$ and $B$ are independent events, then the following pairs of events are also independent:
    \begin{enumerate}[(a)]
        \item $A$ and $\comp B$
        \item $\comp A$ and $B$
        \item $\comp A$ and $\comp B$
    \end{enumerate}\bigskip
    \begin{itemize}
        \item Proof of (a)
        \item[] Setup: Want to show 
        \item[] One way:\vspace{170pt}
        \item[] Another way using conditional probabilities: \vfill
        \item Similar logic can be used to prove (b), and (a) and (b) imply (c).
    \end{itemize}\newpage
    \item How independence relates to the other relationships for events.
    \begin{itemize}
        \item These special cases of independence involve probabilities of zero. To check these, we have to use the most general definition of independence \\ $P(A \cap B) = P(A) \cdot P(B)$ because it can handle these cases.\vspace{40pt}
        \item[] This is why the definition of independence needed the additional restrictions when checking $P(A \mid B) = P(A)$ and $P(B \mid A) = P(B)$. 
    \end{itemize}\bigskip
    \begin{enumerate}[(a)]
        \item If $P(A) = 0$ or $P(B) = 0$, the definition of independence always holds.
        \item[] Assume $P(A) = 0$.\vspace{120pt}
        \item[] Similar logic for if $P(B) = 0 \Longrightarrow A \ind B$.\\
        \item If $A$ and $B$ are mutually exclusive, show if $A$ and $B$ are independent.
        \item[] Need $P(A \cap B) = P(A) \cdot P(B)$ for independence:\vspace{100pt}
        \item[] If events are mutually exclusive, there is a very \blankul{3cm} relationship (if one event occurs, the other \blankul{2cm} occur).\bigskip
        \item If $B \subset A$, show if $A$ and $B$ are independent.
        \item[] Need $P(A \cap B) = P(A) \cdot P(B)$ for independence.\vspace{140pt}
    \end{enumerate}
\end{itemize}\bigskip

Extending the independence definition
\begin{itemize}
    \item Before extending the definition of independence to more than two events, let's see an example.
    \item Example: A jar contains four marbles numbered 1, 2, 3, and 4. One marble is to be drawn at random. Let the events $A$, $B$, and $C$ be defined by $A = \{1, 2\}$, $B = \{1, 3\}$ and $C = \{1, 4\}$.
    \begin{enumerate}[(a)]
        \item Check if the pairs are independent.\vspace{100pt}
        \item Check if all three are independent.\vspace{80pt}
    \end{enumerate}
    \item[] This shows that $A$, $B$, and $C$ are \blankul{2.5cm} \textbf{independent}, but not \blankul{2.5cm} \textbf{independent}.\bigskip
    \item Definition: Events $A$, $B$, and $C$ are \textbf{mutually independent} if and only if they are pairwise independent (i.e. $(A, B)$, $(A, C)$ and $(B, C)$ are independent pairs) \ul{and} if \\$P(A \cap B \cap C) = P(A) P(B) P(C)$.
\end{itemize}\newpage

Examples\bigskip
\begin{enumerate}
    \item Suppose that $A$, $B$ and $C$ are mutually independent events and that $P(A) = 0.5$, $P(B) = 0.8$ and $P(C) = 0.9$. Find the following probabilities:
    \begin{enumerate}
        \item All three events occur.\vspace{40pt}
        \item Exactly two of the three events occur\vspace{120pt}
        \item None of the events occur\vspace{140pt}
    \end{enumerate}
    \item Let $A$, $B$ and $C$ be (mutually) independent events such that $P(A) = 0.5$, $P(B) = 0.6$ and $P(C) = 0.1$. Calculate $P(\comp{A} \cup \comp{B} \cup C)$.\vspace{140pt}    
\end{enumerate}

\end{document}